<!doctype html>
<html lang="fr">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>AI Lab — RAG Glassbox & Diff Explorer</title>
  <meta name="color-scheme" content="dark light">
  <style>
    :root{
      --bg:#0a1220; --card:#0c1628; --muted:#9fb0c9; --text:#eef3f8; --acc:#63e; --ok:#2ecc71; --warn:#f39c12; --bad:#e74c3c;
      --border: #1a2a44;
    }
    body{margin:0;background:var(--bg);color:var(--text);font:15px/1.5 ui-sans-serif,system-ui,Segoe UI,Roboto,Helvetica,Arial}
    a{color:#9ed0ff}
    .shell{max-width:1200px;margin:24px auto;padding:0 16px}
    header h1{margin:0 0 6px}
    header p{margin:0 0 16px;color:var(--muted)}
    .bar{display:flex;gap:8px;flex-wrap:wrap;align-items:center;margin:12px 0 20px}
    input,select,button,textarea{background:#0f1b30;color:var(--text);border:1px solid var(--border);border-radius:8px;padding:8px 10px}
    input::placeholder,textarea::placeholder{color:#7e8aa3}
    button{cursor:pointer}
    button.primary{background:linear-gradient(90deg,#6c4bff,#4a74ff);border:none}
    .tabs{display:flex;gap:8px;margin:4px 0 16px}
    .tab{padding:8px 12px;border:1px solid var(--border);border-radius:999px;cursor:pointer;color:var(--muted)}
    .tab.active{background:#15223a;color:var(--text);border-color:#2b3b61}
    .panel{display:none}
    .panel.active{display:block}
    .card{background:var(--card);border:1px solid var(--border);border-radius:14px;padding:16px;margin:10px 0}
    .grid{display:grid;gap:12px}
    .grid-2{grid-template-columns:1fr 1fr}
    @media (max-width:1000px){.grid-2{grid-template-columns:1fr}}
    .split{display:grid;grid-template-columns:1.1fr 1.4fr;gap:12px;min-height:520px}
    .box{background:#0f1b30;border:1px solid var(--border);border-radius:12px;padding:12px;overflow:auto}
    .box h4{margin:0 0 8px}
    .timeline{font-family:ui-monospace,Consolas,monospace;font-size:13px;max-height:240px;overflow:auto}
    .tstep{padding:6px 8px;border-left:3px solid #2b3b61;margin:6px 0;background:#0f1b30;border-radius:8px}
    .tstep.ok{border-color:var(--ok)} .tstep.warn{border-color:var(--warn)} .tstep.bad{border-color:var(--bad)}
    .pill{display:inline-block;border:1px solid var(--border);border-radius:999px;padding:2px 8px;margin:2px 4px 0 0;background:#13213a;color:#cfe0ff;font-size:12px}
    .score{height:10px;background:#0f1b30;border:1px solid var(--border);border-radius:999px;overflow:hidden}
    .score > i{display:block;height:100%;background:linear-gradient(90deg,#ff5f6d,#ffc371)}
    .badges{display:flex;gap:8px;flex-wrap:wrap}
    .badge{padding:4px 8px;border-radius:8px;background:#14203a;border:1px solid var(--border)}
    .badge.ok{background:#0f2a1a;border-color:#245a3b}
    .badge.low{background:#2a1c0f;border-color:#5a3d24}
    .muted{color:var(--muted)}
    .small{font-size:12px}
    .mono{font-family:ui-monospace,Consolas,monospace}
    .diff{white-space:pre-wrap;font-family:ui-monospace,Consolas,monospace}
    .ins{background:#12371e} .del{background:#3a1520; text-decoration: line-through}
    .footer-note{color:#94a6c7;font-size:12px;margin-top:10px}
    details summary{cursor:pointer}
  </style>

  <!-- pdf.js sans SRI -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
  <script>pdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';</script>
  <!-- diff-match-patch -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/diff_match_patch/20121119/diff_match_patch.min.js"></script>
</head>
<body>
  <div class="shell">
    <header>
      <h1>AI Lab — RAG Glassbox & Diff Explorer</h1>
      <p>
        Deux mini-outils pédagogiques, qui utilisent <b>votre</b> endpoint Vercel pour appeler un LLM.
        Tout est transparent et expliqué.
      </p>
    </header>

    <!-- Barre de config commune -->
    <div class="card">
      <div class="bar">
        <label>Model
          <select id="model">
            <option value="gpt-5">gpt-5 (par défaut via ton API)</option>
            <option value="gpt-4o-mini">gpt-4o-mini</option>
            <option value="gpt-4o">gpt-4o</option>
          </select>
        </label>
        <label class="small">Clé OpenAI perso (optionnel)
          <input id="userKey" type="password" placeholder="sk-..." style="min-width:260px">
        </label>
        <span class="small muted">Route API : <code class="mono" id="routeHint"></code></span>
      </div>
      <div class="small muted">
        Par défaut, on passe par votre fonction Vercel <code>/api/llm/chat</code> (clé côté serveur).
        Si vous mettez une clé perso, l’appel est fait avec <b>votre</b> clé (jamais stockée).
      </div>
      <div class="small" style="margin-top:6px;">
        <div><b>1) RAG Glassbox</b> vous montre comment une réponse basée sur document est produite (extraction → découpe → sélection → réponse justifiée).</div>
        <div><b>2) Diff Explorer</b> vous montre si un prompt/code réécrit produit vraiment une meilleure sortie sur le même modèle.</div>
      </div>
    </div>

    <!-- Onglets -->
    <div class="tabs">
      <div class="tab active" data-tab="rag">1) RAG Glassbox — “Explique-moi ta réponse”</div>
      <div class="tab" data-tab="diff">2) Code & Prompt Diff Explorer</div>
    </div>

    <!-- Panel RAG -->
    <section class="panel active" id="panel-rag">
      <div class="card grid grid-2">
        <div>
          <h3>Document & Question</h3>
          <div class="bar">
            <input type="file" id="pdfFile" accept="application/pdf">
            <input id="question" style="flex:1" placeholder="Ex. « Quels sont les objectifs du projet ? »">
            <button class="primary" id="runRag">Run RAG</button>
          </div>
          <details class="card">
            <summary><b>Ce que vous allez voir</b></summary>
            <ul>
              <li><b>Lecture</b> : le PDF est lu dans le navigateur.</li>
              <li><b>Découpage</b> : le texte est découpé en « morceaux » (chunks).</li>
              <li><b>Classement</b> : on mesure quels morceaux parlent le plus de votre question.</li>
              <li><b>Réponse du modèle</b> : le LLM ne peut s’appuyer que sur ces morceaux → vous voyez lesquels.</li>
              <li><b>But pédagogique</b> : comprendre que la qualité d’une réponse dépend <u>beaucoup</u> de ce qu’on lui a donné.</li>
            </ul>
          </details>

          <div class="card">
            <h4>Timeline détaillée (RAG)</h4>
            <div id="timeline" class="timeline"></div>
          </div>
        </div>

        <div class="split">
          <div class="box" id="leftPane">
            <h4>Passages sélectionnés</h4>
            <div id="passages" class="small"></div>
            <hr>
            <h4>Infos</h4>
            <div id="ragInfo" class="small muted"></div>
          </div>
          <div class="box" id="rightPane">
            <h4>Réponse du modèle</h4>
            <div id="answer"></div>
            <div class="badges" id="badges"></div>
            <div class="footer-note">Les références <b>[1]</b>, <b>[2]</b> renvoient aux passages du panneau gauche.</div>
          </div>
        </div>
      </div>
    </section>

    <!-- Panel Diff -->
    <section class="panel" id="panel-diff">
      <div class="card">
        <h3>Code & Prompt Diff Explorer (avec test réel)</h3>

        <!-- 1. Entrée -->
        <div class="grid grid-2">
          <div class="box">
            <h4>1. Votre prompt / code d’origine</h4>
            <textarea id="src" rows="6" placeholder="Ex. « Fais un dessin de chien en ASCII »"></textarea>
            <div class="small muted">
              Mettez ici la version “simple” ou “trop courte”.
            </div>
          </div>
          <div class="box">
            <h4>2. Objectif</h4>
            <input id="goal" placeholder="Ex. Rendre le prompt plus précis, ajouter le style, demander plusieurs versions…">
            <button class="primary" id="improve">Améliorer avec l’IA</button>
            <div class="small muted" id="improveStatus"></div>
          </div>
        </div>

        <!-- 2. Résultat IA + diff -->
        <div class="grid grid-2" style="margin-top:14px;">
          <div class="box">
            <h4>Prompt amélioré (IA)</h4>
            <textarea id="dst" rows="6" placeholder="La version réécrite apparaîtra ici…"></textarea>
          </div>
          <div class="box">
            <h4>Diff texte (optionnel)</h4>
            <div id="diff" class="diff small"></div>
          </div>
        </div>

        <!-- 3. Test réel -->
        <div class="card" style="margin-top:14px;">
          <h4>3. Tester les deux sur le même modèle</h4>
          <p class="small muted">
            On envoie <b>deux requêtes</b> à votre API : l’une avec le prompt original, l’autre avec le prompt amélioré.
            Avec <code>gpt-5</code> ça peut prendre 20–40 s ×2 → la timeline ci-dessous vous dit où on en est.
          </p>
          <div class="bar">
            <input id="ctx" style="flex:1" placeholder="Contexte de test (facultatif), ex. « Le sujet porte sur le jeu d’osselets antique »">
            <button class="primary" id="runTest">Lancer le test sur les 2 prompts</button>
          </div>
          <div class="grid grid-2">
            <div class="box">
              <h4>Sortie avec prompt original</h4>
              <div id="outOrig" class="small"></div>
            </div>
            <div class="box">
              <h4>Sortie avec prompt amélioré</h4>
              <div id="outImproved" class="small"></div>
            </div>
          </div>
          <details style="margin-top:10px;">
            <summary>Afficher aussi la timeline (prompts)</summary>
            <div id="timeline-diff" class="timeline"></div>
          </details>
        </div>

        <div class="small muted" style="margin-top:10px;">
          Remarque : cette version appelle un endpoint “chat” → elle produit du texte.
          Pour générer <b>des images</b>, il faudrait un endpoint dédié images (DALL·E, etc.) côté Vercel.
        </div>
      </div>
    </section>
  </div>

<script>
/** ====== CONFIG ====== **/
const API_ROUTE = '/api/llm/chat';
document.getElementById('routeHint').textContent = API_ROUTE;

/** ====== ONGLET UI ====== **/
document.querySelectorAll('.tab').forEach(t=>{
  t.onclick = () => {
    document.querySelectorAll('.tab').forEach(x=>x.classList.remove('active'));
    document.querySelectorAll('.panel').forEach(x=>x.classList.remove('active'));
    t.classList.add('active');
    document.getElementById('panel-'+t.dataset.tab).classList.add('active');
  };
});

/** ====== OUTILS COMMUNS ====== **/
function estTokens(txt){ return Math.max(1, Math.round((txt||'').length/4)); }
function sanitize(s){ return (s||'').toString().replace(/[<>&]/g, m => ({'<':'&lt;','>':'&gt;','&':'&amp;'}[m])); }

/** appel LLM générique */
async function callLLM(messages, model, extraHeaders = {}) {
  const headers = {'Content-Type':'application/json', ...extraHeaders};
  const userKey = document.getElementById('userKey').value.trim();
  if (userKey) headers['x-user-api-key'] = userKey;

  // ne PAS envoyer temperature quand c'est gpt-5 chez toi
  const payload = { model, messages };
  const res = await fetch(API_ROUTE, {
    method:'POST',
    headers,
    body: JSON.stringify(payload)
  });
  const raw = await res.text();
  if (!res.ok) {
    throw new Error(`API ${API_ROUTE} → ${res.status}: ${raw}`);
  }
  try { return JSON.parse(raw); } catch { return {choices:[{message:{content:raw}}]}; }
}

/** timeline RAG */
function logStepRAG(html, cls=''){ 
  const el = document.createElement('div');
  el.className = `tstep ${cls}`;
  el.innerHTML = html;
  const root = document.getElementById('timeline');
  root.appendChild(el);
  root.scrollTop = root.scrollHeight;
}
function resetTimelineRAG(){ document.getElementById('timeline').innerHTML=''; }

/** timeline DIFF */
function logStepDiff(html, cls=''){ 
  const el = document.createElement('div');
  el.className = `tstep ${cls}`;
  el.innerHTML = html;
  const root = document.getElementById('timeline-diff');
  root.appendChild(el);
  root.scrollTop = root.scrollHeight;
}
function resetTimelineDiff(){ document.getElementById('timeline-diff').innerHTML=''; }

/** ====== RAG GLASSBOX ====== **/
async function pdfToText(file){
  const start = performance.now();
  const buf = await file.arrayBuffer();
  const pdf = await pdfjsLib.getDocument({data:buf}).promise;
  logStepRAG(`📄 PDF chargé — ${pdf.numPages} page(s)`, 'ok');
  let text = '';
  for (let i=1; i<=pdf.numPages; i++){
    const page = await pdf.getPage(i);
    const c = await page.getTextContent();
    const t = c.items.map(it=>it.str).join(' ');
    text += '\n' + t;
  }
  const dur = Math.round(performance.now() - start);
  logStepRAG(`✂️ Extraction texte ~${text.length.toLocaleString()} caractères en ${dur} ms (moy/page ~${Math.round(text.length/pdf.numPages)} chars)`, 'ok');
  return text.replace(/\s+\n/g,'\n').replace(/\n{2,}/g,'\n\n');
}

function chunkText(text, opts={win:900, overlap:150}){
  const clean = text.replace(/\r/g,'');
  const parts = [];
  let i = 0;
  while (i < clean.length){
    const s = i, e = Math.min(clean.length, i+opts.win);
    let slice = clean.slice(s, e);
    const extra = clean.slice(e, e+120).match(/^[^.!?]*[.!?]/);
    if (extra) slice += extra[0];
    parts.push({id: parts.length+1, start:s, end:s+slice.length, text:slice.trim()});
    i += (opts.win - opts.overlap);
  }
  return parts;
}

const STOP = new Set('a,à,au,aux,le,la,les,un,une,des,de,du,d,et,ou,mais,que,qui,quoi,pour,par,avec,sans,sur,sous,ces,ce,cette,son,sa,ses,leurs,leur,est,sont,été,être,ainsi,plus,moins,très,trop,ne,pas,ni,comme,car,donc,si,lors,afin,chez,entre,vers,contre,selon,chez,quand,ou,où'.split(','));
function tokenize(s){
  return s
    .toLowerCase()
    .normalize('NFD').replace(/[\u0300-\u036f]/g,'')
    .replace(/[^a-z0-9\s]/g,' ')
    .split(/\s+/).filter(w=>w && !STOP.has(w));
}
function tfidfRank(query, chunks){
  const q = tokenize(query);
  const df = new Map();
  for (const ch of chunks){
    const seen = new Set(tokenize(ch.text));
    for (const t of seen) df.set(t, (df.get(t)||0)+1);
  }
  const N = chunks.length;
  const qset = new Set(q);
  for (const ch of chunks){
    const terms = tokenize(ch.text);
    const tf = new Map();
    for (const t of terms) tf.set(t, (tf.get(t)||0)+1);
    let score = 0;
    for (const t of qset){
      const idf = Math.log( (N+1) / ((df.get(t)||0)+1) );
      score += (tf.get(t)||0) * idf;
    }
    ch.score = score;
  }
  return chunks.sort((a,b)=>b.score-a.score);
}
function renderPassages(list, topk){
  const root = document.getElementById('passages');
  root.innerHTML = '';
  list.slice(0, topk).forEach((ch, idx)=>{
    const div = document.createElement('div');
    div.className='card';
    div.innerHTML = `<div class="pill">#${idx+1} (chunk ${ch.id}) — score ${ch.score.toFixed(2)}</div>
      <div class="small">${sanitize(ch.text)}</div>`;
    root.appendChild(div);
  });
}
function groundingScore(answer, passages){
  const a = new Set(tokenize(answer));
  const p = new Set(tokenize(passages.map(p=>p.text).join(' ')));
  let inter = 0;
  for (const t of a) if (p.has(t)) inter++;
  const g = a.size ? inter / a.size : 0;
  return Math.max(0, Math.min(1, g));
}

document.getElementById('runRag').onclick = async ()=>{
  try{
    resetTimelineRAG();
    const file = document.getElementById('pdfFile').files[0];
    const q = document.getElementById('question').value.trim();
    if (!file) { alert('Choisissez un PDF'); return; }
    if (!q) { alert('Écrivez une question'); return; }

    logStepRAG('🧩 Lecture du PDF & extraction texte…');
    const text = await pdfToText(file);

    logStepRAG('🧩 Chunking (fenêtre ~900, overlap 150)…');
    const chunks = chunkText(text);
    logStepRAG(`🔍 ${chunks.length} chunks générés (avg ~${Math.round(text.length/chunks.length)} chars) en 0 ms.`,'ok');

    logStepRAG('🧮 TF-IDF & ranking…');
    const ranked = tfidfRank(q, chunks);
    const top = ranked.slice(0, 6);
    renderPassages(ranked, 6);
    const ctx = top.map((c,i)=>`[${i+1}] ${c.text}`).join('\n\n');

    const sys = `You are a careful RAG assistant. Answer ONLY using the provided SOURCES.
If the answer is not fully supported, explain what is missing and ask for clarification.
Cite sources like [1], [2].`;
    const user = `QUESTION:\n${q}\n\nSOURCES:\n${ctx}`;

    const model = document.getElementById('model').value;
    const inTok = estTokens(sys+user);
    logStepRAG(`📦 Contexte construit (≈${inTok} tokens estimés).`, 'ok');

    const t0 = performance.now();
    logStepRAG(`🤖 Appel LLM (${model})…`);
    const resp = await callLLM([
      {role:'system', content: sys},
      {role:'user', content: user}
    ], model);
    const ms = Math.round(performance.now() - t0);
    const content = resp?.choices?.[0]?.message?.content || '(no content)';
    logStepRAG(`✅ Réponse reçue en ${ms} ms (≈${estTokens(content)} tokens)`, 'ok');

    document.getElementById('answer').innerHTML = `<div class="card small">${sanitize(content).replace(/\[(\d+)\]/g,'<b>[$1]</b>')}</div>`;
    const g = groundingScore(content, top);
    const badges = document.getElementById('badges');
    badges.innerHTML = '';
    const b1 = document.createElement('div'); b1.className='badge '+(g>0.45?'ok':(g>0.2?'':'low'));
    b1.textContent = g>0.45? 'Bien ancré dans le doc' : (g>0.2? 'Partiellement ancré' : 'Ancrage faible');
    badges.appendChild(b1);

    const info = document.getElementById('ragInfo');
    info.innerHTML = `
      <div>Chunks retenus : ${top.map((c,i)=>`[#${i+1}→${c.id}]`).join(' ')}</div>
      <div class="score" title="Grounding score"><i style="width:${(g*100).toFixed(0)}%"></i></div>
      <div class="muted">Score = recouvrement lexical simple, juste pour illustrer.</div>
    `;

  }catch(e){
    logStepRAG('⛔ '+sanitize(e.message||e), 'bad');
    alert(e.message||e);
  }
};

/** ====== DIFF EXPLORER ====== **/
document.getElementById('improve').onclick = async ()=>{
  const src = document.getElementById('src').value.trim();
  const goal = document.getElementById('goal').value || 'Rendre le prompt plus clair, plus détaillé, avec un exemple, en français.';
  const model = document.getElementById('model').value;
  if (!src){ alert('Mettez un prompt à gauche.'); return; }
  document.getElementById('improveStatus').textContent = '⏳ Génération du prompt amélioré…';
  try{
    const sys = `You improve user prompts or short pieces of code.
Return a JSON object with exactly:
{ "code": "...improved prompt...", "why": "short French explanation" }
No markdown fences.`;
    const user = `Objectif: ${goal}
=== PROMPT ORIGINAL ===
${src}`;

    const out = await callLLM([
      {role:'system', content: sys},
      {role:'user', content: user}
    ], model);
    let payload = out?.choices?.[0]?.message?.content || '';
    payload = payload.trim().replace(/^```json\s*/,'').replace(/```$/,'');
    let obj;
    try{ obj = JSON.parse(payload); }
    catch{ throw new Error('Réponse non-JSON, réessayez.'); }

    const dst = obj.code || '';
    document.getElementById('dst').value = dst;
    document.getElementById('improveStatus').textContent = '✅ Prompt amélioré généré. Vous pouvez lancer le test sur les 2.';

    // Diff texte
    if (window.diff_match_patch) {
      const dmp = new diff_match_patch();
      const diffs = dmp.diff_main(src, dst);
      dmp.diff_cleanupSemantic(diffs);
      const html = diffs.map(([op, text])=>{
        text = sanitize(text);
        if (op===1) return `<span class="ins">${text}</span>`;
        if (op===-1) return `<span class="del">${text}</span>`;
        return text;
      }).join('');
      document.getElementById('diff').innerHTML = html;
    } else {
      document.getElementById('diff').innerHTML = '<span class="muted">diff_match_patch non chargé.</span>';
    }

  }catch(e){
    document.getElementById('improveStatus').textContent = '⛔ '+(e.message||e);
    alert(e.message||e);
  }
};

document.getElementById('runTest').onclick = async ()=>{
  const src = document.getElementById('src').value.trim();
  const dst = document.getElementById('dst').value.trim();
  const model = document.getElementById('model').value;
  const ctx = document.getElementById('ctx').value.trim();
  if (!src){ alert('Pas de prompt original.'); return; }
  if (!dst){ alert('Générez d’abord le prompt amélioré.'); return; }

  const btn = document.getElementById('runTest');
  btn.disabled = true;
  btn.textContent = '⏳ Test en cours…';
  document.getElementById('outOrig').textContent = '';
  document.getElementById('outImproved').textContent = '';
  resetTimelineDiff();
  logStepDiff('▶️ Lancement du test : 2 appels vont être faits (original + amélioré).');

  try{
    const start = performance.now();
    logStepDiff('📨 Appel 1/2 avec le prompt original…');
    const r1 = await callLLM([
      {role:'user', content: ctx ? (ctx + '\n\n' + src) : src }
    ], model);
    const ms1 = Math.round(performance.now() - start);
    logStepDiff(`✅ Réponse 1/2 reçue en ${ms1} ms.`, 'ok');
    const out1 = r1?.choices?.[0]?.message?.content || '(vide)';
    document.getElementById('outOrig').textContent = out1;

    const start2 = performance.now();
    logStepDiff('📨 Appel 2/2 avec le prompt amélioré…');
    const r2 = await callLLM([
      {role:'user', content: ctx ? (ctx + '\n\n' + dst) : dst }
    ], model);
    const ms2 = Math.round(performance.now() - start2);
    logStepDiff(`✅ Réponse 2/2 reçue en ${ms2} ms.`, 'ok');
    const out2 = r2?.choices?.[0]?.message?.content || '(vide)';
    document.getElementById('outImproved').textContent = out2;

    logStepDiff('🎯 Test terminé. Comparez visuellement les 2 sorties.','ok');

  }catch(e){
    logStepDiff('⛔ '+sanitize(e.message||e), 'bad');
    alert(e.message||e);
  }finally{
    btn.disabled = false;
    btn.textContent = 'Lancer le test sur les 2 prompts';
  }
};
</script>
</body>
</html>
