{
  "generatedAt": "2026-01-28T06:46:28.298Z",
  "model": "gpt-5",
  "profile": "balanced",
  "threshold": 65,
  "totalAnalyzed": 100,
  "totalPublished": 60,
  "publishTarget": "default",
  "debug": {
    "analysisFailed": true,
    "usedLLM": false,
    "useOpenAIEnv": true,
    "hasOpenAIKey": true,
    "minPublish": 12,
    "weightsUsed": {
      "research": 35,
      "policy": 35,
      "institution": 15,
      "impact": 15
    },
    "labelsUsed": {
      "research": "Recherche",
      "policy": "Politiques",
      "institution": "Institution",
      "impact": "Impact"
    },
    "descUsed": {
      "research": "articles/journaux, conférences, CFP, résultats scientifiques",
      "policy": "lois, régulations, standards, cadres, gouvernance",
      "institution": "communiqués des grandes agences et autorités (UNESCO, OCDE, CNIL, ministères…)",
      "impact": "impact direct pour la classe/enseignants/universités/EdTech"
    },
    "keysUsed": {
      "research": [
        "arxiv",
        "preprint",
        "doi",
        "journal",
        "conference",
        "proceedings",
        "workshop",
        "submission",
        "cfp",
        "acceptance",
        "springer",
        "wiley",
        "nature",
        "frontiers",
        "acm",
        "ieee"
      ],
      "policy": [
        "ai act",
        "regulation",
        "régulation",
        "policy",
        "politique",
        "standard",
        "framework",
        "guidance",
        "law",
        "act",
        "ordonnance",
        "décret",
        "ethics",
        "ethical"
      ],
      "institution": [
        "unesco",
        "oecd",
        "ocde",
        "cnil",
        "edps",
        "ncsc",
        "minist",
        "commission",
        "nsf",
        "ukri",
        "ies",
        "european commission"
      ],
      "impact": [
        "school",
        "education",
        "teacher",
        "enseignant",
        "k-12",
        "universit",
        "student",
        "pupil",
        "mooc",
        "classroom",
        "edtech"
      ]
    }
  },
  "items": [
    {
      "title": "Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering",
      "url": "https://arxiv.org/abs/2601.19847v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-27T17:53:01.000Z",
      "score": 85,
      "summary_en": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS i",
      "summary_fr": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS i",
      "resume_fr": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS i",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Neural Neural Scaling Laws",
      "url": "https://arxiv.org/abs/2601.19831v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-27T17:38:11.000Z",
      "score": 70,
      "summary_en": "Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, we propose Neural Neural Scaling L",
      "summary_fr": "Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, we propose Neural Neural Scaling L",
      "resume_fr": "Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, we propose Neural Neural Scaling L",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "CaseMaster: Designing and Evaluating a Probe for Oral Case Presentation Training with LLM Assistance",
      "url": "https://arxiv.org/abs/2601.19332v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-27T08:17:11.000Z",
      "score": 50,
      "summary_en": "Preparing an oral case presentation (OCP) is a crucial skill for medical students, requiring clear communication of patient information, clinical findings, and treatment plans. However, inconsistent student participation and limited guidance can make this task challenging. While Large Language Models (LLMs) can provide structured content to streamline the process, their role in facilitating skill development and supporting medical education integration remains underexplored. To address this, we conducted a formative study with six medical educators and developed CaseMaster, an interactive prob",
      "summary_fr": "Preparing an oral case presentation (OCP) is a crucial skill for medical students, requiring clear communication of patient information, clinical findings, and treatment plans. However, inconsistent student participation and limited guidance can make this task challenging. While Large Language Models (LLMs) can provide structured content to streamline the process, their role in facilitating skill development and supporting medical education integration remains underexplored. To address this, we conducted a formative study with six medical educators and developed CaseMaster, an interactive prob",
      "resume_fr": "Preparing an oral case presentation (OCP) is a crucial skill for medical students, requiring clear communication of patient information, clinical findings, and treatment plans. However, inconsistent student participation and limited guidance can make this task challenging. While Large Language Models (LLMs) can provide structured content to streamline the process, their role in facilitating skill development and supporting medical education integration remains underexplored. To address this, we conducted a formative study with six medical educators and developed CaseMaster, an interactive prob",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Voice-Based Chatbots for English Speaking Practice in Multilingual Low-Resource Indian Schools: A Multi-Stakeholder Study",
      "url": "https://arxiv.org/abs/2601.19304v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-27T07:44:52.000Z",
      "score": 85,
      "summary_en": "Spoken English proficiency is a powerful driver of economic mobility for low-income Indian youth, yet opportunities for spoken practice remain scarce in schools. We investigate the deployment of a voice-based chatbot for English conversation practice across four low-resource schools in Delhi. Through a six-day field study combining observations and interviews, we captured the perspectives of students, teachers, and principals. Findings confirm high demand across all groups, with notable gains in student speaking confidence. Our multi-stakeholder analysis surfaced a tension in long-term adoptio",
      "summary_fr": "Spoken English proficiency is a powerful driver of economic mobility for low-income Indian youth, yet opportunities for spoken practice remain scarce in schools. We investigate the deployment of a voice-based chatbot for English conversation practice across four low-resource schools in Delhi. Through a six-day field study combining observations and interviews, we captured the perspectives of students, teachers, and principals. Findings confirm high demand across all groups, with notable gains in student speaking confidence. Our multi-stakeholder analysis surfaced a tension in long-term adoptio",
      "resume_fr": "Spoken English proficiency is a powerful driver of economic mobility for low-income Indian youth, yet opportunities for spoken practice remain scarce in schools. We investigate the deployment of a voice-based chatbot for English conversation practice across four low-resource schools in Delhi. Through a six-day field study combining observations and interviews, we captured the perspectives of students, teachers, and principals. Findings confirm high demand across all groups, with notable gains in student speaking confidence. Our multi-stakeholder analysis surfaced a tension in long-term adoptio",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "XR Design Framework for Early Childhood Education",
      "url": "https://arxiv.org/abs/2601.18979v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-26T21:32:35.000Z",
      "score": 85,
      "summary_en": "Extended Reality in early childhood education presents high-risk challenges due to children's rapid developmental changes. While augmented and virtual reality offer immersive pedagogical benefits, they often impose excessive cognitive load or sensory conflict. We introduce the Augmented Human Development (AHD) framework to model these interactions through cognitive, sensory, environmental, and developmental parameters. To ground this framework, we conducted a Systematization of Knowledge (SoK) of 111 peer-reviewed studies involving children aged 3 - 8. Our findings, interpreted through the AHD",
      "summary_fr": "Extended Reality in early childhood education presents high-risk challenges due to children's rapid developmental changes. While augmented and virtual reality offer immersive pedagogical benefits, they often impose excessive cognitive load or sensory conflict. We introduce the Augmented Human Development (AHD) framework to model these interactions through cognitive, sensory, environmental, and developmental parameters. To ground this framework, we conducted a Systematization of Knowledge (SoK) of 111 peer-reviewed studies involving children aged 3 - 8. Our findings, interpreted through the AHD",
      "resume_fr": "Extended Reality in early childhood education presents high-risk challenges due to children's rapid developmental changes. While augmented and virtual reality offer immersive pedagogical benefits, they often impose excessive cognitive load or sensory conflict. We introduce the Augmented Human Development (AHD) framework to model these interactions through cognitive, sensory, environmental, and developmental parameters. To ground this framework, we conducted a Systematization of Knowledge (SoK) of 111 peer-reviewed studies involving children aged 3 - 8. Our findings, interpreted through the AHD",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "IEEE Brings Hands-On STEM Activities to India’s Rural Areas",
      "url": "https://spectrum.ieee.org/ieee-stem-activities-rural-india",
      "source": "IEEE Spectrum – AI",
      "published": "2026-01-26T19:00:03.000Z",
      "score": 85,
      "summary_en": "“Until we get equality in education, we won’t have an equal society.” Spoken by Sonia Sotomayor, associate justice of the U.S. Supreme Court, the words echo sharply across regions of the world where education is not guaranteed. In the far northeastern corner of India—where villages are located in forests, on mountains, and along riverbanks—rural classrooms often operate with limited resources and even fewer opportunities. In districts such as Dhemaji, Assam, and the rural areas of Kharagpur and West Bengal, learning STEM often is just a distant dream. I grew up in rural areas, and I saw how cu",
      "summary_fr": "“Until we get equality in education, we won’t have an equal society.” Spoken by Sonia Sotomayor, associate justice of the U.S. Supreme Court, the words echo sharply across regions of the world where education is not guaranteed. In the far northeastern corner of India—where villages are located in forests, on mountains, and along riverbanks—rural classrooms often operate with limited resources and even fewer opportunities. In districts such as Dhemaji, Assam, and the rural areas of Kharagpur and West Bengal, learning STEM often is just a distant dream. I grew up in rural areas, and I saw how cu",
      "resume_fr": "“Until we get equality in education, we won’t have an equal society.” Spoken by Sonia Sotomayor, associate justice of the U.S. Supreme Court, the words echo sharply across regions of the world where education is not guaranteed. In the far northeastern corner of India—where villages are located in forests, on mountains, and along riverbanks—rural classrooms often operate with limited resources and even fewer opportunities. In districts such as Dhemaji, Assam, and the rural areas of Kharagpur and West Bengal, learning STEM often is just a distant dream. I grew up in rural areas, and I saw how cu",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=15, Impact=0.",
      "image": "https://spectrum.ieee.org/media-library/group-of-higher-secondary-school-students-in-india-holding-up-an-ieee-banner-inside-their-classroom.jpg?id=63278514&width=1200&height=600&coordinates=0%2C98%2C0%2C99"
    },
    {
      "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
      "url": "https://arxiv.org/abs/2601.18795v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-26T18:57:00.000Z",
      "score": 70,
      "summary_en": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. ",
      "summary_fr": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. ",
      "resume_fr": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System",
      "url": "https://arxiv.org/abs/2601.18785v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-26T18:51:20.000Z",
      "score": 70,
      "summary_en": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
      "summary_fr": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
      "resume_fr": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic",
      "url": "https://arxiv.org/abs/2601.18783v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-26T18:50:21.000Z",
      "score": 70,
      "summary_en": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approa",
      "summary_fr": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approa",
      "resume_fr": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approa",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
      "url": "https://arxiv.org/abs/2601.18779v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-26T18:47:21.000Z",
      "score": 70,
      "summary_en": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without impro",
      "summary_fr": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without impro",
      "resume_fr": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without impro",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "From Struggle to Success: Context-Aware Guidance for Screen Reader Users in Computer Use",
      "url": "https://arxiv.org/abs/2601.18092v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-26T03:02:26.000Z",
      "score": 85,
      "summary_en": "Equal access to digital technologies is critical for education, employment, and social participation. However, mainstream interfaces are visually oriented, creating steep learning curves and frequent obstacles for screen reader users, and limiting their independence and opportunities. Existing support is inadequate -- tutorials mainly target sighted users, while human assistance lacks real-time availability. We introduce AskEase, an on-demand AI assistant that provides step-by-step, screen reader user-friendly guidance for computer use. AskEase manages multiple sources of context to infer user",
      "summary_fr": "Equal access to digital technologies is critical for education, employment, and social participation. However, mainstream interfaces are visually oriented, creating steep learning curves and frequent obstacles for screen reader users, and limiting their independence and opportunities. Existing support is inadequate -- tutorials mainly target sighted users, while human assistance lacks real-time availability. We introduce AskEase, an on-demand AI assistant that provides step-by-step, screen reader user-friendly guidance for computer use. AskEase manages multiple sources of context to infer user",
      "resume_fr": "Equal access to digital technologies is critical for education, employment, and social participation. However, mainstream interfaces are visually oriented, creating steep learning curves and frequent obstacles for screen reader users, and limiting their independence and opportunities. Existing support is inadequate -- tutorials mainly target sighted users, while human assistance lacks real-time availability. We introduce AskEase, an on-demand AI assistant that provides step-by-step, screen reader user-friendly guidance for computer use. AskEase manages multiple sources of context to infer user",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "\"Crash Test Dummies\" for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners",
      "url": "https://arxiv.org/abs/2601.18085v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-26T02:47:28.000Z",
      "score": 65,
      "summary_en": "Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI \"simulated learners\" to stress-test and psychometrically characterize assessment pipelines before human use. Objective: Develop an open-source AI virtua",
      "summary_fr": "Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI \"simulated learners\" to stress-test and psychometrically characterize assessment pipelines before human use. Objective: Develop an open-source AI virtua",
      "resume_fr": "Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI \"simulated learners\" to stress-test and psychometrically characterize assessment pipelines before human use. Objective: Develop an open-source AI virtua",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Co-Designing Digital Humans for Online Learning: A Framework for Human-AI Pedagogical Integration",
      "url": "https://arxiv.org/abs/2601.17434v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-24T12:11:26.000Z",
      "score": 85,
      "summary_en": "Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable quality education, these technologies hold promise yet lack clear guidelines for effective design and implementation in online learning. To fill this gap, we introduce a framework specifying when, what, and how digital teachers should be integrated. Our study combines (1) a design space analysis of 87 ",
      "summary_fr": "Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable quality education, these technologies hold promise yet lack clear guidelines for effective design and implementation in online learning. To fill this gap, we introduce a framework specifying when, what, and how digital teachers should be integrated. Our study combines (1) a design space analysis of 87 ",
      "resume_fr": "Artificial intelligence (AI) and large language models (LLMs) are reshaping education, with virtual avatars emerging as digital teachers capable of enhancing engagement, sustaining attention, and addressing instructor shortages. Aligned with the Sustainable Development Goals (SDGs) for equitable quality education, these technologies hold promise yet lack clear guidelines for effective design and implementation in online learning. To fill this gap, we introduce a framework specifying when, what, and how digital teachers should be integrated. Our study combines (1) a design space analysis of 87 ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Do readers prefer AI-generated Italian short stories?",
      "url": "https://arxiv.org/abs/2601.17363v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-24T08:15:13.000Z",
      "score": 65,
      "summary_en": "This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were mo",
      "summary_fr": "This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were mo",
      "resume_fr": "This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were mo",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content",
      "url": "https://arxiv.org/abs/2601.17173v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-23T21:08:02.000Z",
      "score": 85,
      "summary_en": "Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions ",
      "summary_fr": "Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions ",
      "resume_fr": "Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Do We Know What They Know We Know? Calibrating Student Trust in AI and Human Responses Through Mutual Theory of Mind",
      "url": "https://arxiv.org/abs/2601.16960v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-23T18:27:32.000Z",
      "score": 50,
      "summary_en": "Trust and reliance are often treated as coupled constructs in human-AI interaction research, with the assumption that calibrating trust will lead to appropriate reliance. We challenge this assumption in educational contexts, where students increasingly turn to AI for learning support. Through semi-structured interviews with graduate students (N=8) comparing AI-generated and human-generated responses, we find a systematic dissociation: students exhibit high trust but low reliance on human experts due to social barriers (fear of judgment, help-seeking anxiety), while showing low trust but high r",
      "summary_fr": "Trust and reliance are often treated as coupled constructs in human-AI interaction research, with the assumption that calibrating trust will lead to appropriate reliance. We challenge this assumption in educational contexts, where students increasingly turn to AI for learning support. Through semi-structured interviews with graduate students (N=8) comparing AI-generated and human-generated responses, we find a systematic dissociation: students exhibit high trust but low reliance on human experts due to social barriers (fear of judgment, help-seeking anxiety), while showing low trust but high r",
      "resume_fr": "Trust and reliance are often treated as coupled constructs in human-AI interaction research, with the assumption that calibrating trust will lead to appropriate reliance. We challenge this assumption in educational contexts, where students increasingly turn to AI for learning support. Through semi-structured interviews with graduate students (N=8) comparing AI-generated and human-generated responses, we find a systematic dissociation: students exhibit high trust but low reliance on human experts due to social barriers (fear of judgment, help-seeking anxiety), while showing low trust but high r",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias",
      "url": "https://arxiv.org/abs/2601.16934v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-23T17:48:31.000Z",
      "score": 70,
      "summary_en": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our fu",
      "summary_fr": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our fu",
      "resume_fr": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our fu",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems",
      "url": "https://arxiv.org/abs/2601.16890v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-23T16:57:16.000Z",
      "score": 70,
      "summary_en": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the e",
      "summary_fr": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the e",
      "resume_fr": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the e",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Make the Unhearable Visible: Exploring Visualization for Musical Instrument Practice",
      "url": "https://arxiv.org/abs/2601.16708v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-23T12:52:50.000Z",
      "score": 85,
      "summary_en": "We explore the potential of visualization to support musicians in instrument practice through real-time feedback and reflection on their playing. Musicians often struggle to observe the patterns in their playing and interpret them with respect to their goals. Our premise is that these patterns can be made visible with interactive visualization: we can make the unhearable visible. However, understanding the design of such visualizations is challenging: the diversity of needs, including different instruments, skills, musical attributes, and genres, means that any single use case is unlikely to i",
      "summary_fr": "We explore the potential of visualization to support musicians in instrument practice through real-time feedback and reflection on their playing. Musicians often struggle to observe the patterns in their playing and interpret them with respect to their goals. Our premise is that these patterns can be made visible with interactive visualization: we can make the unhearable visible. However, understanding the design of such visualizations is challenging: the diversity of needs, including different instruments, skills, musical attributes, and genres, means that any single use case is unlikely to i",
      "resume_fr": "We explore the potential of visualization to support musicians in instrument practice through real-time feedback and reflection on their playing. Musicians often struggle to observe the patterns in their playing and interpret them with respect to their goals. Our premise is that these patterns can be made visible with interactive visualization: we can make the unhearable visible. However, understanding the design of such visualizations is challenging: the diversity of needs, including different instruments, skills, musical attributes, and genres, means that any single use case is unlikely to i",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
      "url": "https://arxiv.org/abs/2601.16205v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-22T18:56:14.000Z",
      "score": 70,
      "summary_en": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has th",
      "summary_fr": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has th",
      "resume_fr": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has th",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "LLM Prompt Evaluation for Educational Applications",
      "url": "https://arxiv.org/abs/2601.16134v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-22T17:31:25.000Z",
      "score": 50,
      "summary_en": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. T",
      "summary_fr": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. T",
      "resume_fr": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. T",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering",
      "url": "https://arxiv.org/abs/2601.15626v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-22T03:57:47.000Z",
      "score": 85,
      "summary_en": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The st",
      "summary_fr": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The st",
      "resume_fr": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The st",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance",
      "url": "https://arxiv.org/abs/2601.15551v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-22T00:45:15.000Z",
      "score": 85,
      "summary_en": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated kno",
      "summary_fr": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated kno",
      "resume_fr": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated kno",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback",
      "url": "https://arxiv.org/abs/2601.15280v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-21T18:58:08.000Z",
      "score": 50,
      "summary_en": "Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectivenes",
      "summary_fr": "Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectivenes",
      "resume_fr": "Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectivenes",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs",
      "url": "https://arxiv.org/abs/2601.15279v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-21T18:58:01.000Z",
      "score": 85,
      "summary_en": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We",
      "summary_fr": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We",
      "resume_fr": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs",
      "url": "https://arxiv.org/abs/2601.15247v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-21T18:28:31.000Z",
      "score": 70,
      "summary_en": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation fe",
      "summary_fr": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation fe",
      "resume_fr": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation fe",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification",
      "url": "https://arxiv.org/abs/2601.15235v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-21T18:15:47.000Z",
      "score": 70,
      "summary_en": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of",
      "summary_fr": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of",
      "resume_fr": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Prep for the SAT with practice tests in Gemini",
      "url": "https://blog.google/products-and-platforms/products/education/practice-sat-gemini/",
      "source": "Google for Education – Blog",
      "published": "2026-01-21T08:00:00.000Z",
      "score": 50,
      "summary_en": "Gemini now offers full-length, no-cost practice SATs, featuring content from The Princeton Review.",
      "summary_fr": "Gemini now offers full-length, no-cost practice SATs, featuring content from The Princeton Review.",
      "resume_fr": "Gemini now offers full-length, no-cost practice SATs, featuring content from The Princeton Review.",
      "tags": [],
      "breakdown": {
        "research": 0,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=0, Politiques=35, Institution=0, Impact=15.",
      "image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/024-BETT26-PracticeTests-Header-Resize-Social.width-1300.png"
    },
    {
      "title": "IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization",
      "url": "https://arxiv.org/abs/2601.14686v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-21T06:03:05.000Z",
      "score": 85,
      "summary_en": "Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficul",
      "summary_fr": "Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficul",
      "resume_fr": "Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficul",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
      "url": "https://arxiv.org/abs/2601.14249v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-20T18:58:10.000Z",
      "score": 65,
      "summary_en": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that ",
      "summary_fr": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that ",
      "resume_fr": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
      "url": "https://arxiv.org/abs/2601.14243v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-20T18:54:31.000Z",
      "score": 70,
      "summary_en": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demons",
      "summary_fr": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demons",
      "resume_fr": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demons",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
      "url": "https://arxiv.org/abs/2601.14235v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-20T18:46:42.000Z",
      "score": 50,
      "summary_en": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification",
      "summary_fr": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification",
      "resume_fr": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Zero-Shot Speech LLMs for Multi-Aspect Evaluation of L2 Speech: Challenges and Opportunities",
      "url": "https://arxiv.org/abs/2601.16230v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-20T15:48:38.000Z",
      "score": 65,
      "summary_en": "An accurate assessment of L2 English pronunciation is crucial for language learning, as it provides personalized feedback and ensures a fair evaluation of individual progress. However, automated scoring remains challenging due to the complexity of sentence-level fluency, prosody, and completeness. This paper evaluates the zero-shot performance of Qwen2-Audio-7B-Instruct, an instruction-tuned speech-LLM, on 5,000 Speechocean762 utterances. The model generates rubric-aligned scores for accuracy, fluency, prosody, and completeness, showing strong agreement with human ratings within +-2 tolerance,",
      "summary_fr": "An accurate assessment of L2 English pronunciation is crucial for language learning, as it provides personalized feedback and ensures a fair evaluation of individual progress. However, automated scoring remains challenging due to the complexity of sentence-level fluency, prosody, and completeness. This paper evaluates the zero-shot performance of Qwen2-Audio-7B-Instruct, an instruction-tuned speech-LLM, on 5,000 Speechocean762 utterances. The model generates rubric-aligned scores for accuracy, fluency, prosody, and completeness, showing strong agreement with human ratings within +-2 tolerance,",
      "resume_fr": "An accurate assessment of L2 English pronunciation is crucial for language learning, as it provides personalized feedback and ensures a fair evaluation of individual progress. However, automated scoring remains challenging due to the complexity of sentence-level fluency, prosody, and completeness. This paper evaluates the zero-shot performance of Qwen2-Audio-7B-Instruct, an instruction-tuned speech-LLM, on 5,000 Speechocean762 utterances. The model generates rubric-aligned scores for accuracy, fluency, prosody, and completeness, showing strong agreement with human ratings within +-2 tolerance,",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Exploring Learners' Expectations and Engagement When Collaborating with Constructively Controversial Peer Agents",
      "url": "https://arxiv.org/abs/2601.13479v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-20T00:25:33.000Z",
      "score": 50,
      "summary_en": "Peer agents can supplement real-time collaborative learning in asynchronous online courses. Constructive Controversy (CC) theory suggests that humans deepen their understanding of a topic by confronting and resolving controversies. This study explores whether CC's benefits apply to LLM-based peer agents, focusing on the impact of agents' disputatious behaviors and disclosure of agents' behavior designs on the learning process. In our mixed-method study (n=144), we compare LLMs that follow detailed CC guidelines (regulated) to those guided by broader goals (unregulated) and examine the effects ",
      "summary_fr": "Peer agents can supplement real-time collaborative learning in asynchronous online courses. Constructive Controversy (CC) theory suggests that humans deepen their understanding of a topic by confronting and resolving controversies. This study explores whether CC's benefits apply to LLM-based peer agents, focusing on the impact of agents' disputatious behaviors and disclosure of agents' behavior designs on the learning process. In our mixed-method study (n=144), we compare LLMs that follow detailed CC guidelines (regulated) to those guided by broader goals (unregulated) and examine the effects ",
      "resume_fr": "Peer agents can supplement real-time collaborative learning in asynchronous online courses. Constructive Controversy (CC) theory suggests that humans deepen their understanding of a topic by confronting and resolving controversies. This study explores whether CC's benefits apply to LLM-based peer agents, focusing on the impact of agents' disputatious behaviors and disclosure of agents' behavior designs on the learning process. In our mixed-method study (n=144), we compare LLMs that follow detailed CC guidelines (regulated) to those guided by broader goals (unregulated) and examine the effects ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving",
      "url": "https://arxiv.org/abs/2601.13453v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-19T23:11:02.000Z",
      "score": 50,
      "summary_en": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipel",
      "summary_fr": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipel",
      "resume_fr": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipel",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models",
      "url": "https://arxiv.org/abs/2601.13443v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-19T23:00:14.000Z",
      "score": 50,
      "summary_en": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings. We introduce Explicit Cognitive Allocation, a general principle for struc",
      "summary_fr": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings. We introduce Explicit Cognitive Allocation, a general principle for struc",
      "resume_fr": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings. We introduce Explicit Cognitive Allocation, a general principle for struc",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "ICo3D: An Interactive Conversational 3D Virtual Human",
      "url": "https://arxiv.org/abs/2601.13148v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-19T15:30:08.000Z",
      "score": 85,
      "summary_en": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, ",
      "summary_fr": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, ",
      "resume_fr": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation",
      "url": "https://arxiv.org/abs/2601.11792v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-16T21:36:04.000Z",
      "score": 85,
      "summary_en": "Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, ",
      "summary_fr": "Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, ",
      "resume_fr": "Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "On the Probability of First Success in Differential Evolution: Hazard Identities and Tail Bounds",
      "url": "https://arxiv.org/abs/2601.11499v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-16T18:24:24.000Z",
      "score": 50,
      "summary_en": "We study first-hitting times in Differential Evolution (DE) through a conditional hazard frame work. Instead of analyzing convergence via Markov-chain transition kernels or drift arguments, we ex press the survival probability of a measurable target set $A$ as a product of conditional first-hit probabilities (hazards) $p_t=\\Prob(E_t\\mid\\mathcal F_{t-1})$. This yields distribution-free identities for survival and explicit tail bounds whenever deterministic lower bounds on the hazard hold on the survival event. For the L-SHADE algorithm with current-to-$p$best/1 mutation, we construct a checkabl",
      "summary_fr": "We study first-hitting times in Differential Evolution (DE) through a conditional hazard frame work. Instead of analyzing convergence via Markov-chain transition kernels or drift arguments, we ex press the survival probability of a measurable target set $A$ as a product of conditional first-hit probabilities (hazards) $p_t=\\Prob(E_t\\mid\\mathcal F_{t-1})$. This yields distribution-free identities for survival and explicit tail bounds whenever deterministic lower bounds on the hazard hold on the survival event. For the L-SHADE algorithm with current-to-$p$best/1 mutation, we construct a checkabl",
      "resume_fr": "We study first-hitting times in Differential Evolution (DE) through a conditional hazard frame work. Instead of analyzing convergence via Markov-chain transition kernels or drift arguments, we ex press the survival probability of a measurable target set $A$ as a product of conditional first-hit probabilities (hazards) $p_t=\\Prob(E_t\\mid\\mathcal F_{t-1})$. This yields distribution-free identities for survival and explicit tail bounds whenever deterministic lower bounds on the hazard hold on the survival event. For the L-SHADE algorithm with current-to-$p$best/1 mutation, we construct a checkabl",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Extractive summarization on a CMOS Ising machine",
      "url": "https://arxiv.org/abs/2601.11491v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-16T18:14:02.000Z",
      "score": 70,
      "summary_en": "Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports inte",
      "summary_fr": "Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports inte",
      "resume_fr": "Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports inte",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation",
      "url": "https://arxiv.org/abs/2601.11488v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-16T18:09:19.000Z",
      "score": 70,
      "summary_en": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i",
      "summary_fr": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i",
      "resume_fr": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking",
      "url": "https://arxiv.org/abs/2601.11459v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-16T17:34:37.000Z",
      "score": 70,
      "summary_en": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, y",
      "summary_fr": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, y",
      "resume_fr": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, y",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Children's Expectations, Engagement, and Evaluation of an LLM-enabled Spherical Visualization Platform in the Classroom",
      "url": "https://arxiv.org/abs/2601.11060v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-16T07:49:28.000Z",
      "score": 50,
      "summary_en": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language question",
      "summary_fr": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language question",
      "resume_fr": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language question",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions",
      "url": "https://arxiv.org/abs/2601.10951v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-16T02:34:22.000Z",
      "score": 85,
      "summary_en": "The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimen",
      "summary_fr": "The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimen",
      "resume_fr": "The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimen",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
      "url": "https://arxiv.org/abs/2601.10700v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-15T18:54:50.000Z",
      "score": 70,
      "summary_en": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability",
      "summary_fr": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability",
      "resume_fr": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
      "url": "https://arxiv.org/abs/2601.10691v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-15T18:50:48.000Z",
      "score": 50,
      "summary_en": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (stude",
      "summary_fr": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (stude",
      "resume_fr": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (stude",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "On the origin of neural scaling laws: from random graphs to natural language",
      "url": "https://arxiv.org/abs/2601.10684v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-15T18:46:09.000Z",
      "score": 70,
      "summary_en": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scalin",
      "summary_fr": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scalin",
      "resume_fr": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scalin",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies",
      "url": "https://arxiv.org/abs/2601.10660v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-15T18:30:15.000Z",
      "score": 50,
      "summary_en": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning o",
      "summary_fr": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning o",
      "resume_fr": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning o",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "AI Sycophancy: How Users Flag and Respond",
      "url": "https://arxiv.org/abs/2601.10467v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-15T14:51:50.000Z",
      "score": 50,
      "summary_en": "While concerns about LLM sycophancy have grown among researchers and developers, how users themselves experience this behavior remains largely unexplored. We analyze Reddit discussions to investigate how users detect, mitigate, and perceive sycophantic AI. We develop the ODR Framework that maps user experiences across three stages: observing sycophantic behaviors, detecting sycophancy, and responding to these behaviors. Our findings reveal that users employ various detection techniques, including cross-platform comparison and inconsistency testing. We document diverse mitigation approaches, su",
      "summary_fr": "While concerns about LLM sycophancy have grown among researchers and developers, how users themselves experience this behavior remains largely unexplored. We analyze Reddit discussions to investigate how users detect, mitigate, and perceive sycophantic AI. We develop the ODR Framework that maps user experiences across three stages: observing sycophantic behaviors, detecting sycophancy, and responding to these behaviors. Our findings reveal that users employ various detection techniques, including cross-platform comparison and inconsistency testing. We document diverse mitigation approaches, su",
      "resume_fr": "While concerns about LLM sycophancy have grown among researchers and developers, how users themselves experience this behavior remains largely unexplored. We analyze Reddit discussions to investigate how users detect, mitigate, and perceive sycophantic AI. We develop the ODR Framework that maps user experiences across three stages: observing sycophantic behaviors, detecting sycophancy, and responding to these behaviors. Our findings reveal that users employ various detection techniques, including cross-platform comparison and inconsistency testing. We document diverse mitigation approaches, su",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers",
      "url": "https://arxiv.org/abs/2601.10104v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-15T06:17:47.000Z",
      "score": 85,
      "summary_en": "The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \\textbf{3,609} carefully curated que",
      "summary_fr": "The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \\textbf{3,609} carefully curated que",
      "resume_fr": "The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \\textbf{3,609} carefully curated que",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "url": "https://arxiv.org/abs/2601.09708v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-14T18:59:59.000Z",
      "score": 70,
      "summary_en": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align",
      "summary_fr": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align",
      "resume_fr": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Value-Aware Numerical Representations for Transformer Language Models",
      "url": "https://arxiv.org/abs/2601.09706v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-14T18:59:14.000Z",
      "score": 50,
      "summary_en": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into",
      "summary_fr": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into",
      "resume_fr": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Empathy Applicability Modeling for General Health Queries",
      "url": "https://arxiv.org/abs/2601.09696v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-14T18:47:02.000Z",
      "score": 50,
      "summary_en": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We re",
      "summary_fr": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We re",
      "resume_fr": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We re",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
      "url": "https://arxiv.org/abs/2601.09688v1",
      "source": "arXiv – cs.CL (NLP/LLM)",
      "published": "2026-01-14T18:38:31.000Z",
      "score": 70,
      "summary_en": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, appl",
      "summary_fr": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, appl",
      "resume_fr": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, appl",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "url": "https://arxiv.org/abs/2601.09667v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-14T17:57:43.000Z",
      "score": 50,
      "summary_en": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "summary_fr": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "resume_fr": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "url": "https://arxiv.org/abs/2601.09667v2",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-14T17:57:43.000Z",
      "score": 50,
      "summary_en": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "summary_fr": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "resume_fr": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves a",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics",
      "url": "https://arxiv.org/abs/2601.09470v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-14T13:25:44.000Z",
      "score": 50,
      "summary_en": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations b",
      "summary_fr": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations b",
      "resume_fr": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations b",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education",
      "url": "https://arxiv.org/abs/2601.09156v1",
      "source": "arXiv – cs.AI + education/MOOC",
      "published": "2026-01-14T04:51:54.000Z",
      "score": 85,
      "summary_en": "Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation",
      "summary_fr": "Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation",
      "resume_fr": "Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Leveraging learning analytics to enhance immersive teacher simulations: Challenges and opportunities",
      "url": "https://arxiv.org/abs/2601.08954v1",
      "source": "arXiv – cs.HC + education",
      "published": "2026-01-13T19:44:04.000Z",
      "score": 65,
      "summary_en": "This chapter examines how data analytics can be leveraged to enhance immersive teacher simulations, situating this inquiry within the broader learning sciences discourse on embodied cognition, data-informed feedback, and teacher professional learning. It explores both conceptual foundations and empirical cases to illustrate how analytics serve as mediational tools that connect immersive experiences with reflective teaching practice. The chapter unfolds in multiple sections: (1) The Innovation Journey: An Overview of Immersive Teacher Simulations outlines the evolution from traditional simulati",
      "summary_fr": "This chapter examines how data analytics can be leveraged to enhance immersive teacher simulations, situating this inquiry within the broader learning sciences discourse on embodied cognition, data-informed feedback, and teacher professional learning. It explores both conceptual foundations and empirical cases to illustrate how analytics serve as mediational tools that connect immersive experiences with reflective teaching practice. The chapter unfolds in multiple sections: (1) The Innovation Journey: An Overview of Immersive Teacher Simulations outlines the evolution from traditional simulati",
      "resume_fr": "This chapter examines how data analytics can be leveraged to enhance immersive teacher simulations, situating this inquiry within the broader learning sciences discourse on embodied cognition, data-informed feedback, and teacher professional learning. It explores both conceptual foundations and empirical cases to illustrate how analytics serve as mediational tools that connect immersive experiences with reflective teaching practice. The chapter unfolds in multiple sections: (1) The Innovation Journey: An Overview of Immersive Teacher Simulations outlines the evolution from traditional simulati",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 0,
        "institution": 15,
        "impact": 15
      },
      "reason": "Heuristique: Recherche=35, Politiques=0, Institution=15, Impact=15.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
      "url": "https://arxiv.org/abs/2601.08777v1",
      "source": "arXiv – cs.LG + learning analytics",
      "published": "2026-01-13T18:08:06.000Z",
      "score": 70,
      "summary_en": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main ",
      "summary_fr": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main ",
      "resume_fr": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main ",
      "tags": [],
      "breakdown": {
        "research": 35,
        "policy": 35,
        "institution": 0,
        "impact": 0
      },
      "reason": "Heuristique: Recherche=35, Politiques=35, Institution=0, Impact=0.",
      "image": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
    }
  ]
}